{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\prama\\anaconda3\\lib\\site-packages (5.28.0)\n",
      "Requirement already satisfied: torch in c:\\users\\prama\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\prama\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prama\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\prama\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Collecting git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 72958fcd3c98a7afdc61f953aa58c544ebda2f79) to c:\\users\\prama\\appdata\\local\\temp\\pip-req-build-0jv6pmqx\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0.dev0)\n",
      "  Downloading tokenizers-0.14.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     -                                        0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "     ----                                     0.2/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "     -----                                    0.3/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------                                0.5/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------                             0.7/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------                           0.8/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -----------------                        1.0/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------------                     1.1/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "     -----------------------                  1.3/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------                1.4/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------              1.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -----------------------------            1.6/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     --------------------------------         1.8/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -----------------------------------      2.0/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   2.1/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from transformers==4.34.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0.dev0)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "                                              0.0/295.0 kB ? eta -:--:--\n",
      "     -------------                          102.4/295.0 kB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------     266.2/295.0 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 295.0/295.0 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\prama\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers==4.34.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers==4.34.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers==4.34.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers==4.34.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prama\\anaconda3\\lib\\site-packages (from requests->transformers==4.34.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7746408 sha256=f8e62215de24b7e0ca810065876e437f36e048963b9f10116b4803c3a5a6027e\n",
      "  Stored in directory: c:\\users\\prama\\appdata\\local\\pip\\cache\\wheels\\89\\7e\\b3\\04336a04924aa0de029f81a87788444ff8202f8eeeb389c534\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.5\n",
      "    Uninstalling huggingface-hub-0.23.5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\prama\\AppData\\Local\\Temp\\pip-req-build-0jv6pmqx'\n",
      "  Running command git rev-parse -q --verify 'sha^72958fcd3c98a7afdc61f953aa58c544ebda2f79'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
      "  Running command git checkout -q 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\users\\\\prama\\\\anaconda3\\\\scripts\\\\huggingface-cli.exe'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
      "  Cloning https://github.com/casper-hansen/AutoAWQ.git (to revision 1c5ccc791fa2cb0697db3b4070df1813f1736208) to c:\\users\\prama\\appdata\\local\\temp\\pip-req-build-45wtgpgp\n",
      "  Resolved https://github.com/casper-hansen/AutoAWQ.git to commit 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~uggingface-hub (C:\\Users\\prama\\anaconda3\\Lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/casper-hansen/AutoAWQ.git 'C:\\Users\\prama\\AppData\\Local\\Temp\\pip-req-build-45wtgpgp'\n",
      "  Running command git rev-parse -q --verify 'sha^1c5ccc791fa2cb0697db3b4070df1813f1736208'\n",
      "  Running command git fetch -q https://github.com/casper-hansen/AutoAWQ.git 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
      "  Running command git checkout -q 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\prama\\AppData\\Local\\Temp\\pip-req-build-45wtgpgp\\setup.py\", line 93, in <module>\n",
      "          check_dependencies()\n",
      "        File \"C:\\Users\\prama\\AppData\\Local\\Temp\\pip-req-build-45wtgpgp\\setup.py\", line 72, in check_dependencies\n",
      "          raise RuntimeError(\n",
      "      RuntimeError: Cannot find CUDA_HOME. CUDA must be available to build the package.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf\n",
    "!pip install torch transformers\n",
    "!pip3 install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
    "\n",
    "!pip3 install git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~uggingface-hub (C:\\Users\\prama\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement awq (from versions: none)\n",
      "ERROR: No matching distribution found for awq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install awq\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoAWQForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      4\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Mistral-7B-v0.1-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'awq'"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''{prompt}\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))\n",
    "\n",
    "\"\"\"\n",
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
